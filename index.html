<html>

<head>
<meta http-equiv="Content-Language" content="zh-tw">
<meta http-equiv="Content-Type" content="text/html; charset=big5">
<meta name="GENERATOR" content="Microsoft FrontPage 4.0">
<meta name="ProgId" content="FrontPage.Editor.Document">
<title>Tom Ko</title>

<script type="text/javascript">

  var _gaq = _gaq || [];
  _gaq.push(['_setAccount', 'UA-37646687-2']);
  _gaq.push(['_trackPageview']);

  (function() {
    var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
    ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
    var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
  })();

</script>
</head>

<body>

<div id="container">
<div id="logo">
<h1>Tom, Ko Yu Ting  <img src="./images/Tom_head.jpg" align="middle" height="200"/></h1>
</div>

<div id="main">
<a name="bio"></a>

<p>
<i>The LORD said, “If as one people speaking the same language they have begun to do this, then nothing they plan to do will be impossible for them.
Come, let us go down and confuse their language so they will not understand each other.” (Genesis 11: 6-7) </i>
</p>

<p>
This is the reason why there are so many languages in the world. Although it is God's intention to confuse human being with the languages, He still gives us a chance to break the spell. God implies that nothing will be impossible when human being get unified. Thus, I believe that everything is achievable with technology.
</p>






<h3>Announcement</h3>
<p>
<!--  <a href="http://www.noahlab.com.hk/" target=_top>Huawei Noah's Ark Lab </a> -->
I am currently recruiting intern students in the field of speech and natural language processing.
Candidates need to be self-motivated and good at programming (e.g. C++, python).
If you are interested in speech or machine translation related research, please feel free to contact me.
</p>


<P>
<HR>
</P>
<p>

<h3>Contact: tomkocse@gmail.com</h3>

<a href="https://scholar.google.com/citations?user=26-lhTQAAAAJ" target=_top>Tom Ko - Google Scholar</a>.

<a name="bio"></a>
<h3>Biography</h3>
<p>
Tom, Ko Yu Ting received the B.Eng. degree in computer engineering from the Chinese University of Hong Kong in 2003. Then he received the M.Phil. and Ph.D. degree in computer science and engineering from the Hong Kong University of Science and Technology in 2010 and 2014 respectively.
Throughout his postgraduate studies, he was supervised by
<a href="http://www.cs.ust.hk/~mak" target=_top>Professor Brian Mak</a>.
After that, he joined Huawei Noah's Ark Lab as a research scientist.
In 2019, he worked as an assistant professor at
<a href="http://www.sustech.edu.cn/en/" target=_top>Southern University of Science and Technology</a>
in China.
In 2021, he joined
<a href="https://ailab.bytedance.com/" target=_top>ByteDance AI</a>
as a research scientist.
His research interests include speech recognition and natural language processing. 
</p>

<P>
<HR>
</P>
<p>


<a name="bio"></a>
<h3>Education</h3>
<p> <B> Ph.D. </B> in Computer Science and Engineering  in HKUST, 2014 </p> 
<p> <B> M.Phil. </B> in Computer Science and Engineering  in HKUST, 2010 </p> 
<p> <B> M.Sc. </B> in IC Design Engineering  in HKUST, 2007 </p> 
<p> <B> Bachelor Degree </B> in Computer Engineering  in CUHK, 2003 </p> 

<P>
<HR>
</P>
<p>


<a name="publ"></a>
<h3>Publications</h3>
<!-- Format Reference (Conference)
<p>
<b>Author1</b>, Auhtor2, and Author 3,
"Title", in <i>Proc. XXX</i>, year, pages.
</p>
-->
	
<h4>[Reports]</h4>
<UL>

<LI><p>
Shanbo Cheng, Zhichao Huang, Tom Ko, Hang Li, Ningxin Peng, Lu Xu, Qini Zhang <br> 
"<b>Towards Achieving Human Parity on End-to-end Simultaneous Speech Translation via LLM Agent</b>", <br> 
https://arxiv.org/abs/2407.21646
</p>

<LI><p>
Zhichao Huang, Rong Ye, Tom Ko, Qianqian Dong, Shanbo Cheng, Mingxuan Wang, Hang Li <br> 
"<b>Speech Translation with Large Language Models: An Industrial Practice</b>", <br> 
https://arxiv.org/pdf/2312.13585
</p>

	
<h4>[Conference Papers]</h4>
<UL>

<LI><p>
Qiushi Huang, Tom Ko, Zhan Zhuang, Lilian Tang, Yu Zhang <br> 
"<b>HiRA: Parameter-Efficient Hadamard High-Rank Adaptation for Large Language Models</b>", <br> 
in <i> Proceedings of ICLR, April, 2025, Singapore </i>
</p>

<LI><p>
Zhichao Huang, Chutong Meng, Tom Ko <br> 
"<b>RepCodec: A Speech Representation Codec for Speech Tokenization</b>", <br> 
in <i> Proceedings of ACL, August, 2024, Thailand </i>
</p>

<LI><p>
Qiushi Huang, Xubo Liu, Tom Ko, Bo Wu, Wenwu Wang, Yu Zhang, Lilian Tang <br> 
"<b>Selective Prompting Tuning for Personalized Conversations with LLMsSelective Prompting Tuning for Personalized Conversations with LLMs</b>", <br> 
in <i> Proceedings of ACL Findings, August, 2024, Thailand </i>
</p>

<LI><p>
Qianqian Dong, Zhiying Huang, Qiao Tian, Chen Xu, Tom Ko, Yunlong Zhao, Siyuan Feng, Tang Li, Kexin Wang, Xuxin Cheng, Fengpeng Yue, Ye Bai, Xi Chen, Lu Lu, Zejun Ma, Yuping Wang, Mingxuan Wang, Yuxuan Wang <br>
"<b>PolyVoice: Language Models for Speech to Speech Translation</b>", <br> 
in <i> Proceedings of ICLR, May, 2024, Vienna, Austria </i>
</p>

	
<LI><p>
Rong Ye, Chengqi Zhao, Tom Ko, Chutong Meng, Tao Wang, Mingxuan Wang, Jun Cao <br>
"<b>GigaST: A 10,000-hour Pseudo Speech Translation Corpus</b>", <br> 
in <i> Proceedings of Interspeech, August, 2023, Dublin, Ireland </i>
</p>

<LI><p>
Chutong Meng, Junyi Ao, Tom Ko, Mingxuan Wang, Haizhou Li <br>
"<b>CoBERT: Self-Supervised Speech Representation Learning Through Code Representation Learning</b>", <br> 
in <i> Proceedings of Interspeech, August, 2023, Dublin, Ireland </i>
</p>

<LI><p>
Xubo Liu, Qiushi Huang, Xinhao Mei, Haohe Liu, Qiuqiang Kong, Jianyuan Sun, Shengchen Li, Tom Ko, Yu Zhang, Lilian H. Tang, Mark D. Plumbley, Volkan Kılıç, Wenwu Wang <br>
"<b>Visually-Aware Audio Captioning With Adaptive Audio-Visual Attention</b>", <br> 
in <i> Proceedings of Interspeech, August, 2023, Dublin, Ireland </i>
</p>


<LI><p>
"<b>FINDINGS OF THE IWSLT 2023 EVALUATION CAMPAIGN</b>", <br> 
in <i> Proceedings of IWSLT 2023, Canada </i>
</p>

<LI><p>
Chen Xu, Xiaoqian Liu, Xiaowen Liu, Qingxuan Sun, Yuhao Zhang, Murun Yang, Qianqian Dong, Tom Ko, Mingxuan Wang, Tong Xiao, Anxiang Ma, Jingbo Zhu <br>
"<b>CTC-based Non-autoregressive Speech Translation</b>", <br> 
in <i> Proceedings of ACL, July, 2023, Canada </i>
</p>

<LI><p>
Kexin Wang, Yunlong Zhao, Qianqian Dong, Tom Ko, Mingxuan Wang <br>
"<b>MOSPC: MOS Prediction Based on Pairwise Comparison</b>", <br> 
in <i> Proceedings of ACL, July, 2023, Canada </i>
</p>

<LI><p>
Dong Zhang, Rong Ye, Tom Ko, Mingxuan Wang, Yaqian Zhou <br>
"<b>DUB: Discrete Unit Back-translation for Speech Translation</b>", <br> 
in <i> Proceedings of ACL Findings, July, 2023, Canada </i>
</p>


<LI><p>
Chen Xu, Rong Ye, Qianqian Dong, Chengqi Zhao, Tom Ko, Mingxuan Wang, Tong Xiao, Jingbo Zhu <br>
"<b>Recent Advances in Direct Speech-to-text Translation</b>", <br> 
in <i> Proceedings of IJCAI, 2023, Macao </i>
</p>


<LI><p>
Xuxin Cheng, Qianqian Dong, Fengpeng Yue, Tom Ko, Mingxuan Wang, Yuexian Zou <br>
"<b>M3ST: Mix at Three Levels for Speech Translation</b>", <br> 
in <i> Proceedings of ICASSP, June, 2023, Rhodes Island, Greece </i>
</p>


<LI><p>
Yunhao Gou, <b>Tom Ko</b>, Hansi Yang, James Kwok, Yu Zhang and Mingxuan Wang <br>
"<b>Leveraging per Image-Token Consistency for Vision-Language Pre-training</b>", <br> 
in <i> CVPR 2023 </i>
</p>


<LI><p>
Qiushi Huang, Yu Zhang, <b>Tom Ko</b>, Xubo Liu, Bo Wu, Wenwu Wang and Lilian Tang <br>
"<b>Personalized Dialogue Generation with Persona-Adaptive Attention</b>", <br> 
in <i> Proceedings of AAAI, February, 2023, Washington, DC, USA </i>
</p>


<LI><p>
Junyi Ao, Ziqiang Zhang, Long Zhou, Shujie Liu, Haizhou Li, <b>Tom Ko</b>, Lirong Dai, Jinyu Li, Yao Qian and Furu Wei <br>
"<b>Pre-Training Transformer Decoder for End-to-End ASR Model with Unpaired Speech Data</b>", <br> 
in <i> Proceedings of Interspeech, September, 2022, Incheon, Korea </i>
</p>

<LI><p>
Qianqian Dong, Fengpeng Yue, <b>Tom Ko</b>, Mingxuan Wang, Qibing Bai and Yu Zhang <br>
"<b>Leveraging Pseudo-labeled Data to Improve Direct Speech-to-Speech Translation</b>", <br> 
in <i> Proceedings of Interspeech, September, 2022, Incheon, Korea </i>
</p>

<LI><p>
Rui Wang, Qibing Bai, Junyi Ao, Long Zhou, Zhixiang Xiong, Zhihua Wei, Yu Zhang, <b>Tom Ko</b> and Haizhou Li <br>
"<b>LightHuBERT: Lightweight and Configurable Speech Representation Learning with Once-for-All Hidden-Unit BERT</b>", <br> 
in <i> Proceedings of Interspeech, September, 2022, Incheon, Korea </i>
</p>

<LI><p>
Qibing Bai, <b>Tom Ko</b> and Yu Zhang <br>
"<b>A Study of Modeling Rising Intonation in Cantonese Neural Speech Synthesis</b>", <br> 
in <i> Proceedings of Interspeech, September, 2022, Incheon, Korea </i>
</p>

<LI><p>
Junyi Ao, Rui Wang, Long Zhou, Chengyi Wang, Shuo Ren, Yu Wu, Shujie Liu, <b>Tom Ko</b>, Qing Li, Yu Zhang, Zhihua Wei, Yao Qian, Jinyu Li, Furu Wei <br>
"<b>SpeechT5: Unified-Modal Encoder-Decoder Pre-Training for Spoken Language Processing</b>", <br> 
in <i> Proceedings of ACL, May, 2022, Dublin, Ireland </i>
</p>


<LI><p>
Fengpeng Yue, Yan Feng, Lei He, <b>Tom Ko</b>, Yu Zhang <br>
"<b>EXPLORING MACHINE SPEECH CHAIN FOR DOMAIN ADAPTATION</b>", <br> 
in <i> Proceedings of ICASSP, May, 2022, Singapore </i>
</p>

<LI><p>
Rui Wang, Junyi Ao, Long Zhou, Shujie Liu, Zhihua Wei, <b>Tom Ko</b>, Qing Li, Yu Zhang <br>
"<b>MULTI-VIEW SELF-ATTENTION BASED TRANSFORMER FOR SPEAKER RECOGNITION</b>", <br> 
in <i> Proceedings of ICASSP, May, 2022, Singapore </i>
</p>

<LI><p>
Jingsong Wang, Yuxuan He, Chunyu Zhao, Qijie Shao, Wei-Wei Tu, <b>Tom Ko</b>, Hung-yi Lee, Lei Xie <br>
"<b>Auto-KWS 2021 Challenge: Task, Datasets, and Baselines</b>", <br> 
in <i> Proceedings of Interspeech, September, 2021, Brno, Czech Republic </i>
</p>

<LI><p>
Qiushi Huang, <b>Tom Ko</b>, H. Lilian Tang, Xubo Liu, Bo Wu <br>
"<b>Token-Level Supervised Contrastive Learning for Punctuation Restoration</b>", <br> 
in <i> Proceedings of Interspeech, September, 2021, Brno, Czech Republic </i>
</p>

<LI><p>
Yangbin Chen, <b>Tom Ko</b>, Jianping Wang <br>
"<b>A Meta-Learning Approach for User-Defined Spoken Term Classification with Varying Classes and Examples</b>", <br> 
in <i> Proceedings of Interspeech, September, 2021, Brno, Czech Republic </i>
</p>


<LI><p>
Junyi Ao, <b>Tom Ko</b> <br>
"<b>Improving Attention-based End-to-end ASR by Incorporating an N-gram Neural Network</b>", <br> 
in <i> Proceedings of ISCSLP, January, 2021, Hong Kong </i>
</p>

<LI><p>
Fengpeng Yue, <b>Tom Ko</b> <br>
"<b>An Investigation of Positional Encoding in Transformer-based End-to-end Speech Recognition</b>", <br> 
in <i> Proceedings of ISCSLP, January, 2021, Hong Kong </i>
</p>

<LI><p>
Yangbin Chen, <b>Tom Ko</b>, Lifeng Shang, Xiao Chen, Xin Jiang, Qing Li <br>
"<b>An Investigation of Few-Shot Learning in Spoken Term Classification</b>", <br> 
in <i> Proceedings of Interspeech, September, 2020, Shanghai, China </i>
</p>

<LI><p>
Yangbin Chen, YUN MA, <b>Tom Ko</b>, JIANPING WANG, Qing Li <br>
"<b>MetaMix: Improved Meta-Learning with Interpolation based Consistency Regularization</b>", <br> 
in <i> ICPR 2020</i>
</p>


<LI><p>
Jingsong Wang, <b>Tom Ko</b>, Zhen Xu, Xiawei Guo, Souxiang Liu, Wei-Wei Tu, Lei Xie <br>
"<b>AutoSpeech 2020: The Second Automated Machine Learning Challenge for Speech Classification</b>", <br> 
in <i> Proceedings of Interspeech, September, 2020, Shanghai, China </i>
</p>

<LI><p>
<b>Tom Ko</b>, Yangbin Chen, Qing Li <br>
"<b>Prototypical Networks for Small Footprint Text-independent Speaker Verification</b>", <br> 
in <i> Proceedings of ICASSP, May, 2020, Barcelona, Spain </i>
</p>

<LI><p>
Yingke Zhu, <b>Tom Ko</b>, Brian Mak <br>
"<b>Mixup Learning Strategies for Text-independent Speaker Verification</b>", <br> 
in <i> Proceedings of Interspeech, September, 2019, Graz, Austria </i>
</p>

<LI><p>
Yingke Zhu, <b>Tom Ko</b>, David Snyder, Brian Mak, Daniel Povey <br>
"<b>Self-Attentive Speaker Embeddings for Text-Independent Speaker Verification</b>", <br> 
in <i> Proceedings of Interspeech, September, 2018, Hyderabad, India </i>
</p>

<LI><p>
Zhen Qin, <b>Tom Ko</b>, Guangjian Tian <br>
"<b>Long Distance Voice Channel Diagnosis Using Deep Neural Networks</b>", <br> 
in <i> Proceedings of Interspeech, September, 2018, Hyderabad, India </i>
</p>

<LI><p>
<b>Tom Ko</b>, Vijayaditya Peddinti, Daniel Povey,  Michael L. Seltzer, Sanjeev Khudanpur <br>
"<b>A study on data augmentation of reverberant speech for robust speech recognition</b>", <br> 
in <i> Proceedings of ICASSP, March, 2017, New Orleans, USA</i>
</p>
	
<LI><p>
Yajie Miao, Mohammad Gowayyed, Xingyu Na, <b>Tom Ko</b>, Florian Metze, and Alexander Waibel <br>
"<b>An Empirical Exploration of CTC Acoustic Models</b>", <br> 
in <i> Proceedings of ICASSP, March, 2016, Shanghai, China</i>
</p>
	
<LI><p>
Vijayaditya Peddinti, Guoguo Chen, Vimal Manohar, <b>Tom Ko</b>, Daniel Povey, Sanjeev Khudanpur <br>
"<b>JHU ASpIRE system : Robust LVCSR with TDNNs, i-vector adaptation and RNN-LMs</b>", <br>
in <i> IEEE Automatic Speech Recognition and Understanding Workshop (ASRU), December, 2015, Scottsdale, Arizona, USA</i>
</p>
	
<LI><p>
<b>Tom Ko</b>, Vijayaditya Peddinti, Daniel Povey, Sanjeev Khudanpur <br>
"<a href="https://github.com/tomkocse/tomkocse.github.io/raw/master/paper/IS151532.pdf" target=_top>
Audio Augmentation for Speech Recognition</a>", <br> 
in <i> Proceedings of Interspeech, September, 2015, Dresden, Germany </i>
<a href="https://github.com/tomkocse/tomkocse.github.io/raw/master/paper/IS2015-poster.pdf" target=_top>
(poster) </a>
</p>
	
<LI><p>
<b>Tom Ko</b>, Brian Mak, Dongpeng Chen <br>
"<a href="https://github.com/tomkocse/tomkocse.github.io/raw/master/paper/ICASSP2012-0004097.pdf" target=_top>
Modeling Inter-cluster and Intra-cluster Discrimination Among Triphones</a>", <br> 
in <i> Proceedings of the International Symposium of Chinese Spoken Language Processing, September, 2014, Singapore </i>
<a href="https://github.com/tomkocse/tomkocse.github.io/raw/master/paper/ICASSP2012-oral.pptx" target=_top>
(poster) </a>
</p>
	
<LI><p>
<b>Tom Ko</b>, Brian Mak and Cheung-Chi Leung <br>
"<a href="https://github.com/tomkocse/tomkocse.github.io/raw/master/paper/ICASSP2012-0004097.pdf" target=_top>
SUBSPACE GAUSSIAN MIXTURE MODEL WITH STATE-DEPENDENT SUBSPACE DIMENSIONS</a>", <br> 
in <i> Proceedings of ICASSP, pages 1744-1748, May, 2014, Florence, Italy </i>
<a href="https://github.com/tomkocse/tomkocse.github.io/raw/master/paper/ICASSP2012-oral.pptx" target=_top>
(poster) </a>
</p>
	
<LI><p>
<b>Tom Ko</b> and Brian Mak <br>
"<a href="https://github.com/tomkocse/tomkocse.github.io/raw/master/paper/ICASSP2012-0004097.pdf" target=_top>
DERIVATION OF EIGENTRIPHONES BY WEIGHTED PRINCIPAL COMPONENT ANALYSIS</a>", <br> 
in <i> Proceedings of ICASSP, pages 4097-4100, March, 2012, Kyoto, Japan </i> 
<a href="https://github.com/tomkocse/tomkocse.github.io/raw/master/paper/ICASSP2012-oral.pptx" target=_top>
(oral) </a>
</p>
	
<LI><p>
<b>Tom Ko</b> and Brian Mak <br>
"<a href="https://github.com/tomkocse/tomkocse.github.io/raw/master/paper/IS110731.pdf" target=_top>
A FULLY AUTOMATED DERIVATION OF STATE-BASED EIGENTRIPHONES FOR TRIPHONE MODELING WITH NO TIED STATES USING REGULARIZATION</a>", <br> 
in <i> Proceedings of Interspeech, pages 781-784, August, 2011, Florence, Italy </i>
<a href="https://github.com/tomkocse/tomkocse.github.io/raw/master/paper/IS2011-oral.pdf" target=_top>
(oral) </a>
</p>
	
<LI><p>
<b>Tom Ko</b> and Brian Mak <br>
"<a href="https://github.com/tomkocse/tomkocse.github.io/raw/master/paper/ICASSP2011-0004892.pdf" target=_top>
EIGENTRIPHONES: A BASIS FOR CONTEXT-DEPENDENT ACOUSTIC MODELING</a>", <br> 
in <i> Proceedings of ICASSP, pages 4892-4895, May, 2011, Prague, Czech Republic </i> 
<a href="https://github.com/tomkocse/tomkocse.github.io/raw/master/paper/ICASSP2011-poster.pdf" target=_top>
(poster) </a>
</p>
	
<LI><p>
Brian Mak and <b>Tom Ko</b> <br>
"<a href="https://github.com/tomkocse/tomkocse.github.io/raw/master/paper/ISCSLP2010-05684839.pdf" target=_top>
Problems of Modeling Phone Deletion in Conversational Speech for Speech Recognition</a>", <br> 
in <i> Proceedings of the International Symposium of Chinese Spoken Language Processing, pages 114-118, Nov, 2010 ,Taiwan</i>
<a href="https://github.com/tomkocse/tomkocse.github.io/raw/master/paper/ISCSLP2010-oral.ppt" target=_top>
(oral) </a>
</p>
<LI><p>
<b>Tom Ko</b> and Brian Mak <br>
"<a href="https://github.com/tomkocse/tomkocse.github.io/raw/master/paper/ICASSP2010-0004858.pdf" target=_top>
IMPROVING SPEECH RECOGNITION BY EXPLICIT MODELING OF PHONE DELETIONS</a>", <br> 
in <i> Proceedings of ICASSP, pages 4858-4861, March, 2010, Dallas, Texas, USA </i>
<a href="https://github.com/tomkocse/tomkocse.github.io/raw/master/paper/ICASSP2010-poster.pdf" target=_top>
(poster) </a>
</p>
	
<LI><p>
Brian Mak and <b>Tom Ko</b> <br>
"<a href="https://github.com/tomkocse/tomkocse.github.io/raw/master/paper/IS090470.pdf" target=_top>
Automatic Estimation of Decoding Parameters Using Large-Margin Iterative Linear Programming</a>", <br> 
in <i> Proceedings of Interspeech, pages 1219-1222, Sept, 2009, Brighton, U.K. </i>
<a href="https://github.com/tomkocse/tomkocse.github.io/raw/master/paper/IS2009-poster.pdf" target=_top>
(poster) </a>
</p>
	
<LI><p>
Brian Mak and <b>Tom Ko</b> <br>
"<a href="https://github.com/tomkocse/tomkocse.github.io/raw/master/paper/IS080754.pdf" target=_top>
Min-max Discriminative Training of Decoding Parameters Using Iterative Linear Programming</a>", <br> 
in <i> Proceedings of Interspeech, pages 915-918, Sept, 2008, Brisbane, Australia </i>
</p>
<!--
<LI><p>
"<a href="https://github.com/tomkocse/tomkocse.github.io/raw/master/paper/temp.pdf" target=_top>
TEMP</a>", in
</p>
<LI><p>
"<a href="https://github.com/tomkocse/tomkocse.github.io/raw/master/paper/temp.htm" target=_top>
TEMP</a>", in
</p>
<LI><p>
-->

</UL>

<h4>[Journal Papers]</h4>
<UL>
<LI><p>
Xinhao Mei, Chutong Meng, Haohe Liu, Qiuqiang Kong, Tom Ko, Chengqi Zhao, Mark D Plumbley, Yuexian Zou, Wenwu WangXinhao Mei, Chutong Meng, Haohe Liu, Qiuqiang Kong, Tom Ko, Chengqi Zhao, Mark D Plumbley, Yuexian Zou, Wenwu Wang <br>
"<b>Wavcaps: A chatgpt-assisted weakly-labelled audio captioning dataset for audio-language multimodal research</b>", <br> 
<i> IEEE Transactions on Audio, Speech and Language Processing, 2024 </i> </p>

<LI><p>
<b>Tom Ko</b> and Brian Mak <br>
"<b>Eigentrigraphemes for Under-Resourced Languages</b>", <br> 
<i> Speech Communications, volume 56, pages 132-141, January, 2014 </i> </p>

<LI><p>
<b>Tom Ko</b> and Brian Mak <br>
"<b>Eigentriphones for Context-dependent Acoustic Modeling</b>", <br> 
<i> IEEE Transactions on Audio, Speech and Language Processing, volume 21, number 6, pages 1285-1294, 2013 </i> </p>
</UL>

<h4>[Thesis]</h4>
<b><a href="https://github.com/tomkocse/tomkocse.github.io/raw/master/paper/Tom_Mphil_thesis_final.pdf" target=_top>
Phone Deletion Modeling in Speech Recognition</a> (M.Phil.)</b>
<p>
</p>
<b><a href="https://github.com/tomkocse/tomkocse.github.io/raw/master/paper/Tom_Mphil_thesis_final.pdf" target=_top>
Distinct Acoustic Modeling for Automatic Speech Recognition</a> (Ph.D.)</b>

<h4>[Award]</h4>
<b><a href="https://github.com/tomkocse/tomkocse.github.io/raw/master/images/award_pgforum2010.jpg" target=_top>
2nd best presentation in Signal Processing Postgraduate Forum 2010 organized by the IEEE Hong Kong Chapter</a></b>

<p>
</p>


<b><a href="https://github.com/tomkocse/tomkocse.github.io/raw/master/images/BestTAaward.jpg" target=_top>
Professor Samuel Chanson Best Teaching Assistant Award 2011-12</a></b>

<P>
<HR>
</P>
<p>

<a name="service"></a>
<H1>Services</H1>

<h4>[Teaching Assistant]</h4>
<UL>
<LI><p>  Comp102 Computer and Programming Fundamental I </p></LI> 
<LI><p>  Comp103 Computer and Programming Fundamental II </p></LI>
<LI><p>  Comp104 Programming Fundamentals and Methodology </p></LI>
<LI><p>  Comp1022Q Introduction to Computing with Excel VBA </p></LI>
</UL>

<P>
<HR>
</P>
<p>




<!-- <h3>My music label:</h3>
	<p><a href="http://www.youtube.com/watch?v=XD9uhaKFos8" target=_top>Canon Rock</a></p>
-->


</body>

